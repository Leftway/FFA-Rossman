---
title: "Rossman Store Sales "
subtitle: "Group 1 - Exploratory Data Analysis"
author: '1. Kang Jun Han Brandon, 2.Kenny Sim Jun Hong, 3. Lim Yong Chuan,  4. Tan Soon Wei,  5. Teo Xiangquan Martin'
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: 
    tufte_variant: "default"
    self_contained: yes
---
```{r libraries, include = F}
# Install & load relevant libraries. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, 
               ggplot2,
               lubridate, 
               plotly,
               kableExtra,
               Knitr,
               dplyr)

# Create html_df for later stlying
html_df <- function(x){
  kable(x) %>%
   kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
}
```

# Importing and cleaning data
## 1. Import files
``` {r Import, eval = T}
# Import train.csv, test.csv and store.csv
train <- read.csv("train.csv", stringsAsFactors = F)
test <- read.csv("test.csv", stringsAsFactors = F)
store <- read.csv("store.csv", stringsAsFactors = F)
```

## 2. Data Structure
```{r Dimensions Check, include = F}
 str(train)
 str(test)
 str(store)
 dim(train)
 dim(test)
 dim(store)
```
 
```{r Data Structure, eval = T, echo= F }
# Rows and columns of each file
matrix(c("1017209", "9", "41088", "8", "1115", "10"),ncol=2, byrow= TRUE)%>% as.data.frame() %>% `row.names<-`(c("Train", "Test", "Store")) %>% `colnames<-`(c("No. of Rows", "No. of Columns")) %>% html_df
```

## 3. Check for NA 
### NA values for train ###   
```{r NA values - train, eval = T, echo = F}
# Train 
train %>% is.na() %>% colSums() %>% data.frame() %>% `colnames<-`("No. of NAs") %>% html_df
```


### NA values for test ###   
``` {r NA values - test, eval = T, echo = F}
# Test 
test %>% is.na() %>% colSums() %>% data.frame() %>% `colnames<-`("No. of NAs") %>% html_df %>% row_spec(5, bold = T, color = "red")
```


_Observations_:

+ Variable 'Open' should have only two possible values (Open = 1 or Closed = 0), so the 11 NA's should be changed to either 1 or 0.
+ If Open is = 1, but we assume = 0, the error score will increase because of misprediction.
+ If Open is = 0, but we assume = 1, then there's no penalty in scoring as closed stores with 0 sales are not considered in scoring. 

Hence, we will impute 1 into the NA values for the 'Open' variable in the test dataset. 

### NA values for store ###   
```{r NA values - Store, eval = T, echo = F}
# Store
store %>% is.na() %>% colSums() %>% data.frame() %>% `colnames<-`("No. of NAs") %>% html_df
```



## 4. Imputing missing values for test
``` {r Impute, eval = T}
# a. Retrieve records with Open = NA
test %>% filter(is.na(Open)) %>% html_df()

# b. Replace NA with Open = 1
test <- test %>% mutate(Open = replace(Open, is.na(Open),1))

# c. Check if NA has been replaced:
sum(is.na(test$Open))
```



## 5. Convert data types

+ Insert explanation for the type conversion here

``` {r Converting data, eval = T}
#a. Train
train <- train %>% mutate(
  DayOfWeek                 = as.factor(DayOfWeek),
  Date                      = as.Date(Date),
  Open                      = as.factor(Open),
  Promo                     = as.factor(Promo), 
  StateHoliday              = as.factor(StateHoliday),   # Has 4 values!
  SchoolHoliday             = as.factor(SchoolHoliday),
  Day                       = as.integer(format(train$Date, "%d")), # New variable 1
  Month                     = as.integer(format(train$Date, "%m")), # New variable 2
  Year                      = as.integer(format(train$Date, "%Y"))) # New variable 3
str(train)

#b. Test
test <- test %>% mutate(
  DayOfWeek                 = as.factor(DayOfWeek),
  Date                      = as.Date(Date),
  Open                      = as.factor(Open),
  Promo                     = as.factor(Promo),
  StateHoliday              = as.factor(StateHoliday),   # Only 2 values! What're the state holidays?
  SchoolHoliday             = as.factor(SchoolHoliday),
  Day                       = as.integer(format(test$Date, "%d")),  # New variable 1
  Month                     = as.integer(format(test$Date, "%m")),  # New variable 2
  Year                      = as.integer(format(test$Date, "%Y")))  # New variable 3
str(test)

#c. Store
store <- store %>% mutate(
  StoreType                 = as.factor(StoreType),
  Assortment                = as.factor(Assortment),
  Promo2                    = as.factor(Promo2))
str(store)

```


# Exploratory Data Analysis
## 1. Dates
Sales data start from `r min(train$Date)` to `r max(train$Date)`, which spans a total of `r max(train$Date) - min(train$Date)` days or 2 years 7 months.

## 2. Sales - By day of week (Dependent Variable)

__Observations__:
Why are Sunday's sales so low? To check further
```{r Sales - DayofWeek, eval = T }
ggplot(data = train, aes (x= DayOfWeek, y= Sales)) +
geom_bar(stat = "identity")

count <- train %>%   count(DayOfWeek, sort = T)
count
unique(train$DayOfWeek)

```
## 3

## Reserved . Skewedness of data
```{r, eval = T}
#group revenues by quarter


train <- train %>% 
          group_by(Store,year,week) %>%
          mutate(sales.wk = sum(Sales,na.rm = TRUE)) %>%
          ungroup() %>%
          arrange(Store,year,week)

train.wk <- distinct(train[,c("Store","year","week","sales.wk")])

train.wk <- 
  train.wk %>%
  group_by(Store,year) %>%
  mutate(
         sales.wk_gr = sales.wk / lag(sales.wk) - 1,   
         sales.wk_mom = sales.wk / lag(sales.wk, 4) - 1,
         sales.wk_d = sales.wk - lag(sales.wk)
         ) %>%
  ungroup()

sales.wk_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk))
sales.wk_plot + geom_density()

sales.wk_gr_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk_gr))
sales.wk_gr_plot + geom_density()

sales.wk_mom_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk_mom))
sales.wk_mom_plot + geom_density()

sales.wk_d_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk_d))
sales.wk_d_plot + geom_density()



train.dy <- 
  train %>%
  group_by(Store,Date)
  mutate(
         sales.dy_gr = Sales / lag(Sales) - 1,   
         sales.dy_wow = Sales / lag(Sales, 7) - 1,
         sales.wk_d = sales.wk - lag(sales.wk)
         sales.wk_yoy = sales.wk - lag(sales.wk)
         ) %>%
  ungroup()

sales.wk_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk))
sales.wk_plot + geom_density()

sales.wk_gr_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk_gr))
sales.wk_gr_plot + geom_density()

sales.wk_mom_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk_mom))
sales.wk_mom_plot + geom_density()

sales.wk_d_plot <- ggplot(data = train.wk,
            mapping = aes(x = sales.wk_d))
sales.wk_d_plot + geom_density()





counttotal <- test[,c("Store","Id")]
counttotal %>%
  group_by(Store) %>%
  summarise(count = n_distinct(Id))

#####################################################################################33 
# Import train.csv, test.csv and store.csv
train <- read.csv("train.csv", stringsAsFactors = F)
test <- read.csv("test.csv", stringsAsFactors = F)
store <- read.csv("store.csv", stringsAsFactors = F)


train <- train %>%
          mutate(week = week(as.Date(Date)), year = year(as.Date(Date)),
                 week = case_when(
                   week == 1 ~ "01",
                   week == 2 ~ "02",
                   week == 3 ~ "03",
                   week == 4 ~ "04",
                   week == 5 ~ "05",
                   week == 6 ~ "06",
                   week == 7 ~ "07",
                   week == 8 ~ "08",
                   week == 9 ~ "09",
                   week == week ~ as.character(week)
                 ),
                 week_year = paste0(year, "W", week))

test <- test %>%
         mutate(week = week(as.Date(Date)), year = year(as.Date(Date)),
               week = case_when(
                 week == 1 ~ "01",
                 week == 2 ~ "02",
                 week == 3 ~ "03",
                 week == 4 ~ "04",
                 week == 5 ~ "05",
                 week == 6 ~ "06",
                 week == 7 ~ "07",
                 week == 8 ~ "08",
                 week == 9 ~ "09",
                 week == week ~ as.character(week)
               ),
               week_year = paste0(year, "W", week))


#cleaning of missing data in store.csv
store <- store %>%
          mutate(CompetitionOpenSinceMonth=ifelse(is.na(CompetitionOpenSinceMonth),median(CompetitionOpenSinceMonth,na.rm=T), CompetitionOpenSinceMonth),
                 CompetitionOpenSinceYear=ifelse(is.na(CompetitionOpenSinceYear),median(CompetitionOpenSinceYear,na.rm=T), CompetitionOpenSinceYear),
                 Promo2SinceYear=ifelse(is.na(Promo2SinceYear),0, Promo2SinceYear),
                 Promo2SinceWeek=ifelse(is.na(Promo2SinceWeek),0, Promo2SinceWeek))


#calculate average sales by store
train.df <- train %>% 
            group_by(week_year, Store) %>%
            mutate(store_avg=mean(Sales, rm.na=T)) %>%
            ungroup()

#select the first average sales data for each store, week_year
train.df <- train.df %>%
  group_by(week_year,Store) %>%
  dplyr::slice(1) %>%
  select(week_year,Store,store_avg,week) %>%
  ungroup()

#############################################################

#OHE for PromoInterval
library(stringr)
store <- cbind.data.frame(store,str_split_fixed(store$PromoInterval, ",", 12))
colnames(store)[11:22] <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")

store$Jan <- ifelse(grepl("Jan",store$PromoInterval),1,0)
store$Feb <- ifelse(grepl("Feb",store$PromoInterval),1,0)
store$Mar <- ifelse(grepl("Mar",store$PromoInterval),1,0)
store$Apr <- ifelse(grepl("Apr",store$PromoInterval),1,0)
store$May <- ifelse(grepl("May",store$PromoInterval),1,0)
store$Jun <- ifelse(grepl("Jun",store$PromoInterval),1,0)
store$Jul <- ifelse(grepl("Jul",store$PromoInterval),1,0)
store$Aug <- ifelse(grepl("Aug",store$PromoInterval),1,0)
store$Sep <- ifelse(grepl("Sep",store$PromoInterval),1,0)
store$Oct <- ifelse(grepl("Oct",store$PromoInterval),1,0)
store$Nov <- ifelse(grepl("Nov",store$PromoInterval),1,0)
store$Dec <- ifelse(grepl("Dec",store$PromoInterval),1,0)
store <- store[,-23]

train.dy <- 
  train.df %>%
  group_by(Store) %>%
  mutate(
         sales_gr = store_avg / lag(store_avg) - 1,
         sales_d = store_avg - lag(store_avg),
         sales_yoy = store_avg/lag(store_avg,53) -1
         )

multi_lag <- function(train.dy, lags, var, ext=""){
  lag_names <- paste0(var,ext,lags)
  lag_funs <- setNames(paste("dplyr::lag(.,",lags,")"), lag_names)
  train.dy %>% group_by(Store) %>% mutate_at(vars(var), funs_(lag_funs)) %>% ungroup()
}

corr_store_avg <- multi_lag(train.dy, 1:53, "store_avg")[,-c(1:7)]
corr_sales_d <- multi_lag(train.dy, 1:53, "sales_d")[,-c(1:7)]
corr_sales_gr <- multi_lag(train.dy, 1:53, "sales_gr")[,-c(1:7)]

corr_store_avg[mapply(is.infinite, corr_store_avg)] <- NA  
corr_sales_d[mapply(is.infinite, corr_sales_d)] <- NA  
corr_sales_gr[mapply(is.infinite, corr_sales_gr)] <- NA  
  
train$norm <- sapply(train$Sales, scale)
sales.daily_plot <- ggplot(data = train,
            mapping = aes(x = Sales))
sales.daily_plot + geom_density()


sales.weekly_plot <- ggplot(data = train.dy,
            mapping = aes(x = store_avg))
sales.weekly_plot + geom_density()

sales_gr_plot <- ggplot(data = train.dy,
            mapping = aes(x = sales_gr))
sales_gr_plot + geom_density()

sales_d_plot <- ggplot(data = train.dy,
            mapping = aes(x = sales_d))
sales_d_plot + geom_density()

sales_d_plot <- ggplot(data = train.dy,
            mapping = aes(x = sales_yoy))
sales_d_plot + geom_density()



ggplot(train.dy,aes(as.numeric(week),store_avg)) +
  geom_bar(stat = "identity")

ggplot(train.dy,aes(as.numeric(week),sales_gr)) +
  geom_bar(stat = "identity")

ggplot(train.dy,aes(as.numeric(week),sales_d)) +
  geom_bar(stat = "identity")

ggplot(train.dy,aes(as.numeric(week),sales_yoy)) +
  geom_bar(stat = "identity")

        
plot <- train.dy %>%
  ggplot(aes(y=store_avg, x=sales_d, color=factor(as.numeric(week)))) +
  labs(title="[Fiscal] Relationship Between Last Quarter Revenue and Revenue", x="Last Quarter Revenue", y="Revenue") +
  geom_point()+
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)

#Make the plot interactive
ggplotly(plot)  # No need to change this line

cor(corr_store_avg[,unlist(lapply(corr_store_avg, is.numeric))],
    use="complete.obs")

cor(corr_sales_gr[,unlist(lapply(corr_sales_gr, is.numeric))],
    use="complete.obs")

cor(corr_sales_d[,unlist(lapply(corr_sales_d, is.numeric))],
    use="complete.obs")

#plot by mean daily sales, graph doesn't make sense
train %>%
  group_by(DayOfWeek, Store) %>%
  mutate(sales = mean(Sales)) %>%
  slice(1) %>%
  ungroup() %>%
  ggplot(aes(y=sales, x=DayOfWeek, color=factor(Store))) +
  geom_line() + xlab("Week") + ylab("Daily Sales") +
  theme(legend.position = "none")

#plot by mean weekly sales, graph shows seasonality
train %>%
  group_by(as.numeric(week), Store) %>%
  mutate(sales = mean(Sales)) %>%
  slice(1) %>%
  ungroup() %>%
  ggplot(aes(y=sales, x=as.numeric(week), color=factor(Store))) +
  geom_line() + xlab("Week") + ylab("Weekly Sales") +
  theme(legend.position = "none")

#plot by mean weekly sales, graph shows seasonality
train.dy %>%
  group_by(as.numeric(week), Store) %>%
  ungroup() %>%
  ggplot(aes(y=store_avg, x=as.numeric(week), color=factor(Store))) +
  geom_line() + xlab("Week") + ylab("Weekly Sales") +
  theme(legend.position = "none")

ggplot(train.dy,aes(as.numeric(week),sales_d)) +
  geom_bar(stat = "identity")

#plot by mean weekly sales, graph shows seasonality
train.dy %>%
  group_by(as.numeric(week), Store) %>%
  ungroup() %>%
  ggplot(aes(y=sales_d, x=as.numeric(week), color=factor(Store))) +
  geom_line() + xlab("Week") + ylab("Weekly Sales") +
  theme(legend.position = "none")

##################################################33

qplot(store_avg -lag(store_avg), data = train.df, bins = 50, main = "Right skewed distribution")
qplot(sales_d, data = train.dy, bins = 50, main = "Right skewed distribution")

qplot(Sales - lag(Sales), data = train, bins = 50, main = "Right skewed distribution")

train.test <- left_join(test, train.avg.df)

#calculate multipliers based on store_avg (and removing NaN and Inf)
train.df$Daily_mult <- train.df$Sales / train.df$store_avg
train.df[!is.finite(train.df$Daily_mult),]$Daily_mult <- NA

#calculate mean by daily-store-storetype-assort and distribute to train.test
train.df <- train.df %>%
  group_by(Store,StoreType,Assortment,DayOfWeek) %>%
  mutate(naive_mean=mean(Sales, rm.na=T)) %>%
  ungroup()

train_wm <- train.df %>%
  group_by(Store,StoreType,Assortment,DayOfWeek) %>%
  slice(1) %>%
  ungroup() %>%
  select(Store,StoreType,Assortment,DayOfWeek,naive_mean)


train.test <- train.test %>% arrange(Store,StoreType,Assortment,DayOfWeek)
train.test <- left_join(train.test,train_wm)

#all the ids are available in training data
table(is.na(train.test$naive_mean))
table(is.na(train$Date))

train.df %>%
  group_by(DayOfWeek, Store) %>%
  mutate(sales=mean(Sales)) %>%
  slice(1) %>%
  ungroup() %>%
  ggplot(aes(y=Sales, x=DayOfWeek, color=factor(Store))) +
  geom_line() + xlab("Day") + ylab("Sales for Store (StoreType,Assortment)") +
  theme(legend.position = "none")

##regression
#merge store data with train.df
#calculate average sales by store
train.final <- merge(train,train.dy, by=c("Store","week_year"))
train.final <- merge(train,store, by="Store")
index.col <- c(3,18)

colSums(is.na(store))

index.col <- c(2,4,14,15,17,26)
train.final <- train.final[,-index.col]
results <- store %>% filter(is.na(CompetitionDistance))

#build a unique ID
#store, week, open, promo, StateHoliday, SchoolHoliday, storetype, Assortment, Promo2, Jan-Dec
#convert characters to factors
train.final[sapply(train.final, is.character)] <- lapply(train.final[sapply(train.final, is.character)], 
                                       as.factor)

ss_id <- c(train.final$Store, as.numeric(train.final$week.x), 
           train.final$Open, train.final$Promo, 
           train.final$StateHoliday, train.final$SchoolHoliday,
           train.final$StoreType, train.final$Assortment,
           train.final$Promo2, train.final$Jan,
           train.final$Feb,train.final$Mar,
           train.final$Apr,train.final$May,
           train.final$Jun,train.final$Jul,
           train.final$Aug,train.final$Sep,
           train.final$Oct,train.final$Nov,
           train.final$Dec) 

ss_id <- unique(ss_id)

ss_id <- data.frame(id=ss_id)
ss_id$ss_id <- factor(ss_id$id)

library(lfe)
mod1 <- felm(sales_d ~ , | data=train.df)


# XGBoost Model Setup
library(xgboost)
set.seed(786354)

# These params take some work to pin down
params <- list(max_depth=5,
               eta=0.2,
               gamma=10,
               min_child_weight = 20,
               objective = "reg:linear")

train.fff<-na.omit(train.final)

xgbCV <- xgb.cv(params=params,
                data=data.matrix(train.fff),
                label=train.fff$sales_d,
                nrounds=100,
                eval_metric="rmse",
                nfold=10,
                stratified=TRUE)

numTrees <- min(which(xgbCV$evaluation_log$test_auc_mean == 
                      max(xgbCV$evaluation_log$test_auc_mean)))

fit4 <- xgboost(params=params,
                data = data.matrix(testing.df[, -8]),
                label = testing.df$RISK,
                nrounds = numTrees,
                eval_metric="rmse")

# Display relative importance of variables for prediction
xgb.train.data = xgb.DMatrix(data.matrix(training.df[, -8]), label = training.df$RISK, missing = NA)
xgb.test.data = xgb.DMatrix(data.matrix(testing.df[, -8]), label = testing.df$RISK, missing = NA)
col_names = attr(xgb.train.data, ".Dimnames")[[2]]
imp = xgb.importance(col_names, fit4)
print("Model Importance")
xgb.plot.importance(imp)

colSums(is.na(train.final))

```
          
